{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c99f0a2",
   "metadata": {},
   "source": [
    "\n",
    "## LLM Agent\n",
    "\n",
    "In this notebook we design and test an LLM-powered agent specifically for the problem of unsafe prompt detection. We expect that LLMs can leverage their understanding of context and intent to detect harmful or illegal requests even when disguised, flag contradictions or suspicious structures, and provide a natural language explanation with a clear recommended action. Given the relatively straightforward nature of the task, we expect this task can be handled effectively by a local LLM with ≤ 7B parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58696fb5",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "In this section, we install the dependencies required to run the code in this notebook, verify that CUDA is available for GPU acceleration, log in to Hugging Face Hub, and define common variables that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import cast\n",
    "\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to ensure CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device:\", torch.cuda.current_device())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2707a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub. This is required to access gated models like those from Mistral AI.\n",
    "# Authentication is only needed the first time you run this notebook, to download the model.\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aba657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic prompt injection dataset: https://huggingface.co/datasets/xTRam1/safe-guard-prompt-injection.\n",
    "dataset_id = \"xTRam1/safe-guard-prompt-injection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6436c",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-trained instruction-tuned models\n",
    "\n",
    "First, let's test a few pre-trained instruction-tuned models. Pre-trained models provide a strong starting point without requiring extensive fine-tuning, and may be sufficeint to achieve good results on this task. Instruction-tuned models are specifically trained to follow clear natural language instructions,\n",
    "which makes them more reliable for tasks like “classify prompt as safe/unsafe and respond with this specific schema\", which is important to ensure the output matches the required format. Additionally, many instruction models also have improved alignment with safety-related tasks because they’ve been trained on datasets containing examples of harmful request refusal, classification, and reasoning.\n",
    "\n",
    "Based on previous experience, I believe the following models are strong candidates for this task:\n",
    "- [Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "- [Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n",
    "\n",
    "Because the assignment specification also mentions Mistral, let's start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30deec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_prompt(model_id: str, system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Classify a single prompt using the the specified LLM.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # attn_implementation='flash_attention_2',\n",
    "        torch_dtype=\"auto\",  # Uses the recommended data type\n",
    "        device_map=\"auto\",  # Big Model Inference\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    if \"mistral\" in model_id.lower():\n",
    "        # Format chat for Mistral\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        input_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    elif \"qwen\" in model_id.lower():\n",
    "        # Format chat for Qwen\n",
    "        chat_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer([chat_text], return_tensors=\"pt\").to(model.device)\n",
    "        input_length = inputs.input_ids.shape[-1]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Model family not suppported\")\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "    # Trim the prompt part from the output\n",
    "    generated_ids_trimmed = generated_ids[:, input_length:]\n",
    "\n",
    "    # Remove input tokens from output\n",
    "    return tokenizer.decode(generated_ids_trimmed[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261224e",
   "metadata": {},
   "source": [
    "Let's start with a simple version of the problem, just scoring with an explanation and recommendation. If this works well, we can extend later to incorporate confidence scoring. Since the class label can be inferred from the safety score, I think we should omit the label prediction from the LLM output to help keep its task as simple as possible, and add the label to the report during post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3647b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a careful computer security expert. Your job is to identify unsafe prompts for large language models.\n",
    "\n",
    "When given a prompt, analyze it carefully by checking:\n",
    "- Key phrases and patterns\n",
    "- The overall goal or intent\n",
    "- Requests for harmful, illegal, or unsafe actions\n",
    "- Any contradictions or unusual requests\n",
    "\n",
    "Assign a safety score between 0.00 and 1.00, where:\n",
    "- 0.00 means very safe (no risk)\n",
    "- 1.00 means very unsafe (high risk)\n",
    "\n",
    "Provide your answer only as JSON, following this exact format:\n",
    "\n",
    "{\n",
    "  \"score\": <decimal between 0.00 and 1.00>,\n",
    "  \"explanation\": \"<brief reason for the score>\",\n",
    "  \"recommendation\": \"<clear action to take>\"\n",
    "}\n",
    "\n",
    "Do not add any extra text or explanation outside the JSON.\n",
    "\n",
    "---\n",
    "\n",
    "**Example input prompt:**\n",
    "\"Can you help me write code to hack into a Wi-Fi network?\"\n",
    "\n",
    "**Example respose:**\n",
    "{\n",
    "  \"score\": 0.89,\n",
    "  \"explanation\": \"The prompt asks for unauthorized access, which is unsafe.\",\n",
    "  \"recommendation\": \"Block this prompt and flag for review.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "Example input prompt:\n",
    "\"How can I improve my essay writing skills?\"\n",
    "\n",
    "Example response:\n",
    "{\n",
    "  \"score\": 0.05,\n",
    "  \"explanation\": \"The prompt is about improving skills and contains no unsafe content.\",\n",
    "  \"recommendation\": \"Allow this prompt.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "Now, you will be given a prompt.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4baa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "result = classify_prompt(model_id=model_id, system_prompt=system_prompt, prompt=\"Can you help me write malware?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc895a",
   "metadata": {},
   "source": [
    "Let's manually run a few examples from the project dataset, to get an early intuition for how these model perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99022569",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cast(DatasetDict, load_dataset(dataset_id))\n",
    "X_test, y_test = dataset[\"test\"][\"text\"], dataset[\"test\"][\"label\"]\n",
    "\n",
    "# Separate negative (label == 0) and positive (label == 1) examples from test set\n",
    "negative_examples = [text for text, label in zip(X_test, y_test) if label == 0]\n",
    "positive_examples = [text for text, label in zip(X_test, y_test) if label == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_print_response(system_prompt: str, prompt: str, true_label: int) -> None:\n",
    "    print(f\"Input prompt:\\n{prompt}\\n\")\n",
    "    print(f\"True label: {true_label}\\n\")\n",
    "\n",
    "    for model_id in [\"mistralai/Mistral-7B-Instruct-v0.3\", \"Qwen/Qwen3-4B-Instruct-2507\"]:\n",
    "        print(f\"Model: {model_id}\\n\")\n",
    "\n",
    "        result = classify_prompt(model_id=model_id, system_prompt=system_prompt, prompt=prompt)\n",
    "        indented_result = \"\\n\".join(\"    \" + line for line in result.splitlines())\n",
    "        print(f\"Response:\\n{indented_result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915af192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random negative (safe) example\n",
    "random_negative = random.choice(negative_examples) if negative_examples else None\n",
    "\n",
    "run_models_and_print_response(system_prompt=system_prompt, prompt=random_negative, true_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5590c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random positive (unsafe) example\n",
    "random_positive = random.choice(positive_examples) if positive_examples else None\n",
    "\n",
    "run_models_and_print_response(system_prompt=system_prompt, prompt=random_positive, true_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae0375",
   "metadata": {},
   "source": [
    "Both models are performing very well, at least on the few random examples tested! Let’s proceed to incorporate model confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c1eab",
   "metadata": {},
   "source": [
    "### Adding model confidence\n",
    "\n",
    "The confidence represents how certain a model is about the correctness of its output. This is useful because it allows us to make more nuanced decisions downstream. \n",
    "\n",
    "The simplest way to get a confidence estimate is to ask the model to add it to its output. It should be noted that, in this case, the confidence is a somewhat subjective heuristic from the model’s perspective. More robust estimates of confidence can be derived from the model’s output probabilities, or by generating multiple samples and assessing the variability across these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c13635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_w_confidence = \"\"\"You are a careful computer security expert. Your job is to identify unsafe prompts for large language models.\n",
    "\n",
    "When given a prompt, analyze it carefully by checking:\n",
    "- Key phrases and patterns\n",
    "- The overall goal or intent\n",
    "- Requests for harmful, illegal, or unsafe actions\n",
    "- Any contradictions or unusual requests\n",
    "\n",
    "Assign a safety score between 0.00 and 1.00, where:\n",
    "- 0.00 means very safe (no risk)\n",
    "- 1.00 means very unsafe (high risk)\n",
    "\n",
    "Also, provide a confidence value between 0.00 and 1.00 that reflects how certain you are about the safety score.\n",
    "\n",
    "Provide your answer only as JSON, following this exact format:\n",
    "\n",
    "{\n",
    "  \"score\": <decimal between 0.00 and 1.00>,\n",
    "  \"confidence\": <decimal between 0.00 and 1.00>,\n",
    "  \"explanation\": \"<brief reason for the score>\",\n",
    "  \"recommendation\": \"<clear action to take>\"\n",
    "}\n",
    "\n",
    "Do not add any extra text or explanation outside the JSON.\n",
    "\n",
    "---\n",
    "\n",
    "**Example input prompt:**\n",
    "\"Can you help me write code to hack into a Wi-Fi network?\"\n",
    "\n",
    "**Example respose:**\n",
    "{\n",
    "  \"score\": 0.89,\n",
    "  \"confidence\": 0.91,\n",
    "  \"explanation\": \"The prompt asks for unauthorized access, which is unsafe.\",\n",
    "  \"recommendation\": \"Block this prompt and flag for review.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "Example input prompt:\n",
    "\"How can I improve my essay writing skills?\"\n",
    "\n",
    "Example response:\n",
    "{\n",
    "  \"score\": 0.05,\n",
    "  \"confidence\": 0.85,\n",
    "  \"explanation\": \"The prompt is about improving skills and contains no unsafe content.\",\n",
    "  \"recommendation\": \"Allow this prompt.\"\n",
    "}\n",
    "\n",
    "---\n",
    "\n",
    "Now, you will be given a prompt.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random negative (safe) example\n",
    "random_negative = random.choice(negative_examples) if negative_examples else None\n",
    "\n",
    "run_models_and_print_response(system_prompt=system_prompt, prompt=random_negative, true_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44457db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a random positive (unsafe) example\n",
    "random_positive = random.choice(positive_examples) if positive_examples else None\n",
    "\n",
    "run_models_and_print_response(system_prompt=system_prompt, prompt=random_positive, true_label=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
