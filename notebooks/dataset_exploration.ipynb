{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd291443",
   "metadata": {},
   "source": [
    "This is a Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84380b38",
   "metadata": {},
   "source": [
    "## Set-Up\n",
    "In this section, we will install the dependencies required to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset import get_project_dataset\n",
    "import statistics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90441a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_project_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65847bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset[\"train\"][\"text\"], dataset[\"train\"][\"label\"]\n",
    "X_test, y_test = dataset[\"test\"][\"text\"], dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_negative, first_positive = None, None\n",
    "\n",
    "# Find the first negative (safe) example (label == 0)\n",
    "for text, label in zip(X_train, y_train):\n",
    "    if label == 0:\n",
    "        first_negative = text\n",
    "        break\n",
    "\n",
    "# Find the first positive (unsafe) example (label == 1)\n",
    "for text, label in zip(X_train, y_train):\n",
    "    if label == 1:\n",
    "        first_positive = text\n",
    "        break\n",
    "\n",
    "print(\"First negative example:\")\n",
    "print(first_negative)\n",
    "\n",
    "print(\"First positive example:\")\n",
    "print(first_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_lengths(texts: Iterable[str]) -> list[int]:\n",
    "    return [len(text.split()) for text in texts]\n",
    "\n",
    "def get_stats(lengths: list[int]) -> dict:\n",
    "    return {\n",
    "        \"Count\": len(lengths),\n",
    "        \"Min\": min(lengths),\n",
    "        \"Max\": max(lengths),\n",
    "        \"Mean\": round(statistics.mean(lengths), 2),\n",
    "        \"Variance\": round(statistics.variance(lengths), 2),\n",
    "    }\n",
    "\n",
    "# Positive and negative texts\n",
    "unsafe_prompts = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "safe_prompts = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "\n",
    "all_lengths = prompt_lengths(X_train)\n",
    "unsafe_lengths = prompt_lengths(unsafe_prompts)\n",
    "safe_lengths = prompt_lengths(safe_prompts)\n",
    "\n",
    "data = {\n",
    "    \"Positive (unsafe)\": get_stats(unsafe_lengths),\n",
    "    \"Negative (safe)\": get_stats(safe_lengths),\n",
    "}\n",
    "df = pd.DataFrame(data).T\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest and longest unsafe prompt\n",
    "shortest_unsafe = min(unsafe_prompts, key=lambda x: len(x.split()))\n",
    "longest_unsafe = max(unsafe_prompts, key=lambda x: len(x.split()))\n",
    "\n",
    "# Shortest and longest safe prompt\n",
    "shortest_safe = min(safe_prompts, key=lambda x: len(x.split()))\n",
    "longest_safe = max(safe_prompts, key=lambda x: len(x.split()))\n",
    "\n",
    "print(\"Shortest unsafe prompt:\\n\", shortest_unsafe)\n",
    "print(\"\\nLongest unsafe prompt:\\n\", longest_unsafe)\n",
    "\n",
    "print(\"\\nShortest safe prompt:\\n\", shortest_safe)\n",
    "print(\"\\nLongest safe prompt:\\n\", longest_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38030283",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 2150, 50)  # bins every 20 words\n",
    "bin_labels = [f\"{b}-{b+50}\" for b in bins[:-1]]\n",
    "\n",
    "unsafe_counts, _ = np.histogram(unsafe_lengths, bins=bins)\n",
    "safe_counts, _ = np.histogram(safe_lengths, bins=bins)\n",
    "\n",
    "# Normalize counts to get proportions\n",
    "unsafe_freqs = unsafe_counts / unsafe_counts.sum()\n",
    "safe_freqs = safe_counts / safe_counts.sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=bin_labels,\n",
    "    y=unsafe_freqs,\n",
    "    name='Positive (unsafe)',\n",
    "    marker_color='red'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=bin_labels,\n",
    "    y=safe_freqs,\n",
    "    name='Negative (safe)',\n",
    "    marker_color='green'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Normalized Prompt Length Distribution by Label',\n",
    "    xaxis_title='Number of Words',\n",
    "    yaxis_title='Proportion',\n",
    "    xaxis_tickangle=-45,\n",
    "    bargap=0.1,\n",
    "    yaxis_type='log'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f74233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea is that unsafe prompts might be more repetitive or formulaic.\n",
    "def shannon_entropy(text: str) -> float:\n",
    "    tokens = text.split()\n",
    "    counts = Counter(tokens)\n",
    "    probs = [count / len(tokens) for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "entropies = [shannon_entropy(t) for t in X_train]\n",
    "\n",
    "# Prepare DataFrame\n",
    "df_entropy = pd.DataFrame({\n",
    "    \"entropy\": entropies,\n",
    "    \"label\": y_train\n",
    "})\n",
    "df_entropy[\"label_name\"] = df_entropy[\"label\"].map({0: \"Safe\", 1: \"Unsafe\"})\n",
    "\n",
    "# Histogram with normalized counts (relative frequencies)\n",
    "fig_hist = px.histogram(\n",
    "    df_entropy, x=\"entropy\", color=\"label_name\",\n",
    "    nbins=30, barmode=\"group\",\n",
    "    opacity=0.6,\n",
    "    histnorm='probability',   # <-- normalize within each class\n",
    "    labels={\"entropy\": \"Shannon Entropy\", \"label_name\": \"Prompt Class\"},\n",
    "    title=\"Normalized Entropy Distribution for Safe vs Unsafe Prompts\",\n",
    "    marginal=\"rug\"  # optional\n",
    ")\n",
    "fig_hist.update_layout(bargap=0.1)\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "# Boxplot distribution\n",
    "# fig_box = px.box(\n",
    "#     df_entropy, x=\"label_name\", y=\"entropy\",\n",
    "#     color=\"label_name\",\n",
    "#     labels={\"label_name\": \"Prompt Class\", \"entropy\": \"Shannon Entropy\"},\n",
    "#     title=\"Entropy Comparison between Safe and Unsafe Prompts\"\n",
    "# )\n",
    "# fig_box.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431833b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(text: str) -> float:\n",
    "    tokens = text.split()\n",
    "    counts = Counter(tokens)\n",
    "    probs = [count / len(tokens) for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "def add_histogram(fig, data, bins, name, color):\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=bin_edges[:-1],\n",
    "        y=counts,\n",
    "        width=bin_width * 0.9,\n",
    "        name=f\"{name} Histogram\",\n",
    "        marker_color=color,\n",
    "        opacity=0.5,\n",
    "    ))\n",
    "    return counts, bin_edges, bin_width\n",
    "\n",
    "def add_normal_fit(fig, data, bin_width, color=\"green\", name=\"Safe Normal Fit\"):\n",
    "    mu, std = norm.fit(data)\n",
    "    x = np.linspace(min(data), max(data), 300)\n",
    "    pdf = norm.pdf(x, mu, std)\n",
    "    pdf_scaled = pdf * len(data) * bin_width\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x,\n",
    "        y=pdf_scaled,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=color, width=3),\n",
    "        name=name\n",
    "    ))\n",
    "\n",
    "def add_gmm_fit(fig, data, bin_width, color=\"red\", name=\"Unsafe GMM Fit\", n_components=2):\n",
    "    # Reshape data for GMM (expects 2D)\n",
    "    data_reshaped = data.reshape(-1, 1)\n",
    "    \n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "    gmm.fit(data_reshaped)\n",
    "    \n",
    "    # Create x-axis range for smooth plot\n",
    "    x = np.linspace(min(data), max(data), 300).reshape(-1, 1)\n",
    "    \n",
    "    # Compute weighted sum of component PDFs for each x\n",
    "    logprob = gmm.score_samples(x)\n",
    "    pdf = np.exp(logprob)\n",
    "    \n",
    "    # Scale PDF to histogram counts\n",
    "    pdf_scaled = pdf * len(data) * bin_width\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x.flatten(),\n",
    "        y=pdf_scaled,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=color, width=3),\n",
    "        name=name\n",
    "    ))\n",
    "\n",
    "# Calculate entropies\n",
    "entropies = [shannon_entropy(t) for t in X_train]\n",
    "df_entropy = pd.DataFrame({\n",
    "    \"entropy\": entropies,\n",
    "    \"label\": y_train\n",
    "})\n",
    "df_entropy[\"label_name\"] = df_entropy[\"label\"].map({0: \"Safe\", 1: \"Unsafe\"})\n",
    "\n",
    "fig = go.Figure()\n",
    "bins = 30\n",
    "\n",
    "safe_data = df_entropy[df_entropy[\"label\"] == 0][\"entropy\"].values\n",
    "unsafe_data = df_entropy[df_entropy[\"label\"] == 1][\"entropy\"].values\n",
    "\n",
    "safe_counts, safe_bin_edges, safe_bin_width = add_histogram(fig, safe_data, bins, \"Safe\", \"green\")\n",
    "unsafe_counts, unsafe_bin_edges, unsafe_bin_width = add_histogram(fig, unsafe_data, bins, \"Unsafe\", \"red\")\n",
    "\n",
    "add_normal_fit(fig, safe_data, safe_bin_width)\n",
    "#add_gmm_fit(fig, unsafe_data, unsafe_bin_width)\n",
    "add_gmm_fit(fig, unsafe_data, unsafe_bin_width, n_components=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entropy Distribution with Normal Fit for Safe and Poisson Fit for Unsafe Prompts\",\n",
    "    xaxis_title=\"Shannon Entropy\",\n",
    "    yaxis_title=\"Count\",\n",
    "    barmode=\"overlay\",\n",
    "    bargap=0.2,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed8381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer and fit on training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa43170",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), \n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f461233",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_tfidf, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
