{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd291443",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "In this notebook, we explore the [Safe-Guard Prompt Injection Dataset](https://huggingface.co/datasets/xTRam1/safe-guard-prompt-injection). The objectives are twofold: first, to understand the project dataset; and second, to identify any trends within the data that we can leverage in our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84380b38",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section, we will install the dependencies required to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from typing import Iterable, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d179499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic prompt injection dataset: https://huggingface.co/datasets/xTRam1/safe-guard-prompt-injection\n",
    "dataset_id = \"xTRam1/safe-guard-prompt-injection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ce156",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "plots_dir = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"docs\", \"content\", \"plots\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90441a",
   "metadata": {},
   "source": [
    "## Preliminary analysis\n",
    "\n",
    "In this section, we loading the dataset from Hugging Face, examining its structure, review the class distribution, and inspecting sample entries. Datasets avaiable on Hugging Face Hub can be accessed using the [`load_dataset()`](https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/loading_methods#datasets.load_dataset) function from the [Datasets](https://pypi.org/project/datasets/) Python library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cast(DatasetDict, load_dataset(\"xTRam1/safe-guard-prompt-injection\"))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65847bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset[\"train\"][\"text\"], dataset[\"train\"][\"label\"]\n",
    "X_test, y_test = dataset[\"test\"][\"text\"], dataset[\"test\"][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56283021",
   "metadata": {},
   "source": [
    "To better understand the dataset, let’s manually inspect a few random examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split negative and positive examples\n",
    "negatives = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "positives = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "\n",
    "# Randomly select one from each class\n",
    "random_negative = random.choice(negatives)\n",
    "random_positive = random.choice(positives)\n",
    "\n",
    "print(\"Random negative example:\")\n",
    "print(random_negative)\n",
    "\n",
    "print(\"\\nRandom positive example:\")\n",
    "print(random_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bded7",
   "metadata": {},
   "source": [
    "Imbalance occurs when one class is significantly more represented. If not properly mitigated, models trained on imbalanced datasets can exhibit bias by favoring the majority class. Let's examine the class distribution to identify any class imablance that could impact our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4de55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter(y_train)\n",
    "\n",
    "label_map = {0: \"safe\", 1: \"unsafe\"}\n",
    "labels = [label_map[label] for label in label_counts.keys()]\n",
    "counts = list(label_counts.values())\n",
    "total = sum(counts)\n",
    "percentages = [100 * c / total for c in counts]\n",
    "\n",
    "fig = px.bar(\n",
    "    x=labels,\n",
    "    y=counts,\n",
    "    text=[f\"{p:.1f}%\" for p in percentages],\n",
    "    labels={\"x\": \"Label\", \"y\": \"Number of prompts\"},\n",
    "    title=\"Label Distribution in Training Set\",\n",
    ")\n",
    "fig.update_xaxes(type=\"category\")\n",
    "fig.update_traces(textposition=\"inside\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28405a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label distribution figure to file for use in the report\n",
    "html_str = pio.to_html(fig, full_html=False, include_plotlyjs=\"cdn\")\n",
    "output_file = os.path.join(plots_dir, \"label_distribution.html\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719800b8",
   "metadata": {},
   "source": [
    "We see here that the dataset is indeed imbalanced, consisting of approximately 70% safe prompts and 30% unsafe prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56d9db",
   "metadata": {},
   "source": [
    "## Text length analysis\n",
    "\n",
    "In this section, we anlalyse the distribution of prompt lengths by class to see, for example, if one prompts of one class are systematically longer than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_lengths(texts: Iterable[str]) -> list[int]:\n",
    "    return [len(text.split()) for text in texts]\n",
    "\n",
    "\n",
    "def get_stats(lengths: list[int]) -> dict:\n",
    "    return {\n",
    "        \"Count\": len(lengths),\n",
    "        \"Min\": min(lengths),\n",
    "        \"Max\": max(lengths),\n",
    "        \"Mean\": round(statistics.mean(lengths), 2),\n",
    "        \"Variance\": round(statistics.variance(lengths), 2),\n",
    "    }\n",
    "\n",
    "\n",
    "# Positive and negative texts\n",
    "unsafe_prompts = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "safe_prompts = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "\n",
    "all_lengths = prompt_lengths(X_train)\n",
    "unsafe_lengths = prompt_lengths(unsafe_prompts)\n",
    "safe_lengths = prompt_lengths(safe_prompts)\n",
    "\n",
    "data = {\n",
    "    \"Positive (unsafe)\": get_stats(unsafe_lengths),\n",
    "    \"Negative (safe)\": get_stats(safe_lengths),\n",
    "}\n",
    "df = pd.DataFrame(data).T\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest and longest unsafe prompt\n",
    "shortest_unsafe = min(unsafe_prompts, key=lambda x: len(x.split()))\n",
    "longest_unsafe = max(unsafe_prompts, key=lambda x: len(x.split()))\n",
    "\n",
    "# Shortest and longest safe prompt\n",
    "shortest_safe = min(safe_prompts, key=lambda x: len(x.split()))\n",
    "longest_safe = max(safe_prompts, key=lambda x: len(x.split()))\n",
    "\n",
    "print(\"Shortest unsafe prompt:\\n\", shortest_unsafe)\n",
    "print(\"\\nLongest unsafe prompt:\\n\", longest_unsafe)\n",
    "\n",
    "print(\"\\nShortest safe prompt:\\n\", shortest_safe)\n",
    "print(\"\\nLongest safe prompt:\\n\", longest_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38030283",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 2150, 50)  # bins every 50 lenghts\n",
    "bin_labels = [f\"{b}-{b+50}\" for b in bins[:-1]]\n",
    "\n",
    "unsafe_counts, _ = np.histogram(unsafe_lengths, bins=bins)\n",
    "safe_counts, _ = np.histogram(safe_lengths, bins=bins)\n",
    "\n",
    "# Normalize counts to get proportions\n",
    "unsafe_freqs = unsafe_counts / unsafe_counts.sum()\n",
    "safe_freqs = safe_counts / safe_counts.sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(x=bin_labels, y=unsafe_freqs, name=\"Positive (unsafe)\", marker_color=\"red\"))\n",
    "\n",
    "fig.add_trace(go.Bar(x=bin_labels, y=safe_freqs, name=\"Negative (safe)\", marker_color=\"green\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"group\",\n",
    "    title=\"Normalized Prompt Length Distribution by Label\",\n",
    "    xaxis_title=\"Number of Words in Prompt\",\n",
    "    yaxis_title=\"Proportion\",\n",
    "    xaxis_tickangle=-45,\n",
    "    bargap=0.2,\n",
    "    yaxis_type=\"log\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=0.98,\n",
    "        y=0.94,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"top\",\n",
    "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot of normalized prompt length distribution by label to file for use in the report\n",
    "html_str = pio.to_html(fig, full_html=False, include_plotlyjs=\"cdn\")\n",
    "output_file = os.path.join(plots_dir, \"prompt_length_distribution.html\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b31b0",
   "metadata": {},
   "source": [
    "It looks like there is nothing meaningful here to help us separate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a31ca3",
   "metadata": {},
   "source": [
    "## Entropy analysis\n",
    "\n",
    "In this section, we calculate Shannon entropy to quantify how diverse or unpredictable the characters or tokens are within prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f74233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea is that unsafe prompts might be more repetitive or formulaic.\n",
    "def shannon_entropy(text: str) -> float:\n",
    "    tokens = text.split()\n",
    "    counts = Counter(tokens)\n",
    "    probs = [count / len(tokens) for count in counts.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "\n",
    "entropies = [shannon_entropy(t) for t in X_train]\n",
    "\n",
    "# Prepare DataFrame\n",
    "df_entropy = pd.DataFrame({\"entropy\": entropies, \"label\": y_train})\n",
    "df_entropy[\"label_name\"] = df_entropy[\"label\"].map({0: \"Safe\", 1: \"Unsafe\"})\n",
    "\n",
    "# Histogram with normalized counts (relative frequencies)\n",
    "fig_hist = px.histogram(\n",
    "    df_entropy,\n",
    "    x=\"entropy\",\n",
    "    color=\"label_name\",\n",
    "    nbins=30,\n",
    "    barmode=\"group\",\n",
    "    opacity=0.6,\n",
    "    histnorm=\"probability\",  # <-- normalize within each class\n",
    "    labels={\"entropy\": \"Shannon Entropy\", \"label_name\": \"Prompt Class\"},\n",
    "    title=\"Normalized Entropy Distribution for Safe vs Unsafe Prompts\",\n",
    "    marginal=\"rug\",  # optional\n",
    ")\n",
    "fig_hist.update_layout(bargap=0.1)\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "# Boxplot distribution\n",
    "# fig_box = px.box(\n",
    "#     df_entropy, x=\"label_name\", y=\"entropy\",\n",
    "#     color=\"label_name\",\n",
    "#     labels={\"label_name\": \"Prompt Class\", \"entropy\": \"Shannon Entropy\"},\n",
    "#     title=\"Entropy Comparison between Safe and Unsafe Prompts\"\n",
    "# )\n",
    "# fig_box.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431833b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_histogram(fig, data, bins, name, color):\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=bin_edges[:-1],\n",
    "            y=counts,\n",
    "            width=bin_width * 0.9,\n",
    "            name=f\"{name}\",\n",
    "            marker_color=color,\n",
    "            opacity=0.5,\n",
    "        )\n",
    "    )\n",
    "    return counts, bin_edges, bin_width\n",
    "\n",
    "\n",
    "def add_normal_fit(fig, data, bin_width, color=\"green\", name=\"Safe Fit (Normal)\"):\n",
    "    mu, std = norm.fit(data)\n",
    "    x = np.linspace(min(data), max(data), 300)\n",
    "    pdf = norm.pdf(x, mu, std)\n",
    "    pdf_scaled = pdf * len(data) * bin_width\n",
    "    fig.add_trace(go.Scatter(x=x, y=pdf_scaled, mode=\"lines\", line=dict(color=color, width=3), name=name))\n",
    "\n",
    "\n",
    "def add_gmm_fit(fig, data, bin_width, color=\"red\", name=\"Unsafe Fit (GMM)\", n_components=2):\n",
    "    # Reshape data for GMM (expects 2D)\n",
    "    data_reshaped = data.reshape(-1, 1)\n",
    "\n",
    "    # Fit GMM\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "    gmm.fit(data_reshaped)\n",
    "\n",
    "    # Create x-axis range for smooth plot\n",
    "    x = np.linspace(min(data), max(data), 300).reshape(-1, 1)\n",
    "\n",
    "    # Compute weighted sum of component PDFs for each x\n",
    "    logprob = gmm.score_samples(x)\n",
    "    pdf = np.exp(logprob)\n",
    "\n",
    "    # Scale PDF to histogram counts\n",
    "    pdf_scaled = pdf * len(data) * bin_width\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=x.flatten(), y=pdf_scaled, mode=\"lines\", line=dict(color=color, width=3), name=name))\n",
    "\n",
    "\n",
    "# Calculate entropies\n",
    "entropies = [shannon_entropy(t) for t in X_train]\n",
    "df_entropy = pd.DataFrame({\"entropy\": entropies, \"label\": y_train})\n",
    "df_entropy[\"label_name\"] = df_entropy[\"label\"].map({0: \"Safe\", 1: \"Unsafe\"})\n",
    "\n",
    "fig = go.Figure()\n",
    "bins = 30\n",
    "\n",
    "safe_data = df_entropy[df_entropy[\"label\"] == 0][\"entropy\"].values\n",
    "unsafe_data = df_entropy[df_entropy[\"label\"] == 1][\"entropy\"].values\n",
    "\n",
    "safe_counts, safe_bin_edges, safe_bin_width = add_histogram(fig, safe_data, bins, \"Safe\", \"green\")\n",
    "unsafe_counts, unsafe_bin_edges, unsafe_bin_width = add_histogram(fig, unsafe_data, bins, \"Unsafe\", \"red\")\n",
    "\n",
    "add_normal_fit(fig, safe_data, safe_bin_width)\n",
    "# add_gmm_fit(fig, unsafe_data, unsafe_bin_width)\n",
    "add_gmm_fit(fig, unsafe_data, unsafe_bin_width, n_components=3)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Entropy Distribution\",\n",
    "    xaxis_title=\"Shannon Entropy\",\n",
    "    yaxis_title=\"Count\",\n",
    "    barmode=\"overlay\",\n",
    "    bargap=0.3,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=0.98,\n",
    "        y=0.94,\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"top\",\n",
    "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e41f9",
   "metadata": {},
   "source": [
    "Here we see that the entropy distribution of safe prompts follows a normal distribution, indicating variation and diversity typical of natural language. However, unsafe prompts are much more likely to exhibit lower entropy values in the range of 3 to 4.5, indicating that these prompts often contain repeated or formulaic wording and less linguistic diversity. However, as indicated by the tail in the unsafe prompt entropy distribution, some unsafe prompts exhibit high entropy values, indicating that they retain considerable linguistic diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot of entropy distribution to file for use in the report\n",
    "html_str = pio.to_html(fig, full_html=False, include_plotlyjs=\"cdn\")\n",
    "output_file = os.path.join(plots_dir, \"entropy_distribution.html\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761abb7",
   "metadata": {},
   "source": [
    "\n",
    "## Exploring n-grams\n",
    "\n",
    "In this section we look at n-grams, which are contiguous sequences of words of length n. The idea is to identify if n-grams separate safe and unsafe prompts and if so, which n-gram lengths best separate safe prompts. We will also inspect a few example n-grams to better understand common phrases in each class.\n",
    "\n",
    "For example, maybe unsafe prompts contain certain phrases like “ignore all previous instructions,” that are exceedingly rare in safe prompts (i.e., “trigger” phrases). Or, maybe unsafe prompts mention certain entities or roles repeatedly. If so, simple n-grams-frequency statistics could be very useful for class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(texts, ngram_range, top_k):\n",
    "    \"\"\"Extract top n-grams from list of texts.\"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    freqs = X.sum(axis=0).A1\n",
    "    ngrams = vectorizer.get_feature_names_out()\n",
    "    freq_df = pd.DataFrame({\"ngram\": ngrams, \"count\": freqs})\n",
    "\n",
    "    return freq_df.sort_values(by=\"count\", ascending=False).head(top_k)\n",
    "\n",
    "\n",
    "# Separate safe and unsafe prompts\n",
    "safe_texts = [text for text, label in zip(X_train, y_train) if label == 0]\n",
    "unsafe_texts = [text for text, label in zip(X_train, y_train) if label == 1]\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    print(f\"Top {n}-grams in safe prompts:\")\n",
    "    top_safe = get_top_ngrams(safe_texts, ngram_range=(n, n), top_k=top_k)\n",
    "    print(top_safe)\n",
    "    print()\n",
    "\n",
    "    print(f\"Top {n}-grams in unsafe prompts:\")\n",
    "    top_unsafe = get_top_ngrams(unsafe_texts, ngram_range=(n, n), top_k=top_k)\n",
    "    print(top_unsafe)\n",
    "    print(\"\\n\" + \"=\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b054c",
   "metadata": {},
   "source": [
    "To get a more objective understand if n-grams are useful for class separation, we can shuffle the labels on the prompts and calculate the average LLR for each shuffle. If the real average LLR is much higher than these shuffled cases, it indicates that the n-grams capture meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffled_avg_llr_test(safe_texts, unsafe_texts, n, top_k=10, num_shuffles=10):\n",
    "    \"\"\"\n",
    "    Compare average top-k LLR of real labels to shuffled labels.\n",
    "\n",
    "    Parameters:\n",
    "    - safe_texts: list of safe prompts (label=0)\n",
    "    - unsafe_texts: list of unsafe prompts (label=1)\n",
    "    - n: int, n-gram length\n",
    "    - top_k: int, number of top LLRs to average\n",
    "    - num_shuffles: int, how many times to shuffle for baseline\n",
    "\n",
    "    Returns:\n",
    "    - real_avg_llr: average top-k LLR with real labels\n",
    "    - shuffle_avg_llrs: list of average top-k LLRs from shuffled labels\n",
    "    \"\"\"\n",
    "\n",
    "    all_texts = safe_texts + unsafe_texts\n",
    "    labels = [0] * len(safe_texts) + [1] * len(unsafe_texts)\n",
    "\n",
    "    # Compute real average LLR\n",
    "    real_avg_llr = get_avg_topk_llr_for_ngram_range(safe_texts, unsafe_texts, n, top_k=top_k)\n",
    "\n",
    "    shuffle_avg_llrs = []\n",
    "    for _ in range(num_shuffles):\n",
    "        random.shuffle(labels)\n",
    "        # Split texts based on shuffled labels\n",
    "        shuffled_safe = [text for text, label in zip(all_texts, labels) if label == 0]\n",
    "        shuffled_unsafe = [text for text, label in zip(all_texts, labels) if label == 1]\n",
    "\n",
    "        # Compute average LLR for shuffled labels\n",
    "        avg_llr_shuffled = get_avg_topk_llr_for_ngram_range(shuffled_safe, shuffled_unsafe, n, top_k=top_k)\n",
    "        shuffle_avg_llrs.append(avg_llr_shuffled)\n",
    "\n",
    "    return real_avg_llr, shuffle_avg_llrs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "real_llr, shuffled_llrs = shuffled_avg_llr_test(safe_texts, unsafe_texts, n=1, top_k=10, num_shuffles=20)\n",
    "\n",
    "print(f\"Real average top-10 LLR: {real_llr:.4f}\")\n",
    "print(f\"Mean shuffled average top-10 LLR: {np.mean(shuffled_llrs):.4f}\")\n",
    "print(f\"Shuffled LLRs range: {min(shuffled_llrs):.4f} - {max(shuffled_llrs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7434e0",
   "metadata": {},
   "source": [
    "To understand how useful different n-gram sizes are at distinguishing safe vs. unsafe prompts, let's plot average top-k log-likelihood ratio (LLR) as a function of n-gram length. Higher average LLR means those n-grams provide stronger, more consistent signals for class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f8756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_llr(k11, k12, k21, k22):\n",
    "    def safe_log(x):\n",
    "        return log(x) if x > 0 else 0\n",
    "\n",
    "    row_sum_1 = k11 + k12\n",
    "    row_sum_2 = k21 + k22\n",
    "    col_sum_1 = k11 + k21\n",
    "    col_sum_2 = k12 + k22\n",
    "    total = row_sum_1 + row_sum_2\n",
    "\n",
    "    E11 = row_sum_1 * col_sum_1 / total\n",
    "    E12 = row_sum_1 * col_sum_2 / total\n",
    "    E21 = row_sum_2 * col_sum_1 / total\n",
    "    E22 = row_sum_2 * col_sum_2 / total\n",
    "\n",
    "    llr = 2 * (\n",
    "        k11 * safe_log(k11 / E11) + k12 * safe_log(k12 / E12) + k21 * safe_log(k21 / E21) + k22 * safe_log(k22 / E22)\n",
    "    )\n",
    "    return llr\n",
    "\n",
    "\n",
    "def get_avg_topk_llr_for_ngram_range(safe_texts, unsafe_texts, n, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute max LLR for n-grams of length n.\n",
    "\n",
    "    Parameters:\n",
    "    - safe_texts: list of safe prompt strings\n",
    "    - unsafe_texts: list of unsafe prompt strings\n",
    "    - n: int, n-gram length\n",
    "\n",
    "    Returns:\n",
    "    - max_llr: float, maximum log-likelihood ratio found for n-grams of length n\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words=\"english\", binary=True)\n",
    "    all_texts = safe_texts + unsafe_texts\n",
    "    X = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    X_safe = X[: len(safe_texts)]\n",
    "    X_unsafe = X[len(safe_texts) :]\n",
    "\n",
    "    llr_scores = []\n",
    "    for i in range(len(features)):\n",
    "        k11 = X_unsafe[:, i].sum()\n",
    "        k12 = X_safe[:, i].sum()\n",
    "        k21 = len(unsafe_texts) - k11\n",
    "        k22 = len(safe_texts) - k12\n",
    "        llr = compute_llr(k11, k12, k21, k22)\n",
    "        llr_scores.append(llr)\n",
    "\n",
    "    llr_scores.sort(reverse=True)\n",
    "    return sum(llr_scores[:top_k]) / top_k\n",
    "\n",
    "\n",
    "def plot_llr_for_ngram_lengths(safe_texts, unsafe_texts, n, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute and plot average top-k LLR for n-gram lengths from 1 to n.\n",
    "\n",
    "    Parameters:\n",
    "    - safe_texts: list of safe prompt strings\n",
    "    - unsafe_texts: list of unsafe prompt strings\n",
    "    - n: int, maximum n-gram length to evaluate\n",
    "    - top_k: int, number of top LLR scores to average (default 10)\n",
    "\n",
    "    Returns:\n",
    "    - None (shows plot)\n",
    "    \"\"\"\n",
    "    avg_llrs = []\n",
    "    labels = []\n",
    "    for i in range(1, n + 1):\n",
    "        avg_llr = get_avg_topk_llr_for_ngram_range(safe_texts, unsafe_texts, i, top_k=top_k)\n",
    "        avg_llrs.append(avg_llr)\n",
    "        if i == 1:\n",
    "            labels.append(\"Unigrams\")\n",
    "        elif i == 2:\n",
    "            labels.append(\"Bigrams\")\n",
    "        elif i == 3:\n",
    "            labels.append(\"Trigrams\")\n",
    "        else:\n",
    "            labels.append(f\"{i}-grams\")\n",
    "\n",
    "    fig = go.Figure(data=[go.Bar(x=labels, y=avg_llrs, marker_color=\"blue\")])\n",
    "    fig.update_layout(\n",
    "        title=f\"Average Top-{top_k} LLR by N-gram Length\",\n",
    "        xaxis_title=\"N-gram Type\",\n",
    "        yaxis_title=\"Average Log-Likelihood Ratio (LLR)\",\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell may take several minutes to run, especially for n-gram lengths of 3 or higher\n",
    "fig = plot_llr_for_ngram_lengths(safe_texts, unsafe_texts, 5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97f0ce",
   "metadata": {},
   "source": [
    "Here we see that unigrams provide the strongest discriminative signal for class separation, and this signal tends to decrease as the n-gram length increases. This means we should focus on unigrams (and maybe bigrams and trigrams) for our TF-IDF-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd16d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save plot of average top-k LLM by n-gram to file for use in the report\n",
    "html_str = pio.to_html(fig, full_html=False, include_plotlyjs=\"cdn\")\n",
    "output_file = os.path.join(plots_dir, \"avg_topk_llr_by_ngram.html\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(html_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
