{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dad73",
   "metadata": {},
   "source": [
    "## Coverage analysis\n",
    "\n",
    "This notebook evaluates and the production system against the test subset of the Safe-Guard Prompt Injection Dataset. This system includes three components: static keyword checker, a TD-IDF classifier, an LLM-based agent.\n",
    "\n",
    "This notebook evaluates and compares the coverage of prompt safety detection across three approaches: a baseline classifier, an LLM-based agent, and a static keyword filter. The goal is to understand the strengths, overlaps, and gaps in these methods, to understand the need for additional pipeline components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03dde3c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section, we install the dependencies required to run the code in this notebook, define common variables used throughout, and add the project root to `PYTHONPATH` so we can use components from the `src/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbacc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126613e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8-noqa-cell\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import cast\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from datasets.arrow_dataset import Column\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from src.analyzers import KeywordChecker, TfidfClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic prompt injection dataset: https://huggingface.co/datasets/xTRam1/safe-guard-prompt-injection.\n",
    "dataset_id = \"xTRam1/safe-guard-prompt-injection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebooks_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "plots_dir = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"docs\", \"content\", \"plots\"))\n",
    "models_dir = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"models\"))\n",
    "src_dir = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"src\"))\n",
    "data_dir = os.path.abspath(os.path.join(notebooks_dir, \"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabfb73",
   "metadata": {},
   "source": [
    "### System evaluation\n",
    "\n",
    "In this section, we evaluate each component on the test subsection of the project dataset, and then combine their predictions to produce a unified classification report for the complete system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project dataset\n",
    "dataset = cast(DatasetDict, load_dataset(dataset_id))\n",
    "X_test, y_test = cast(Column, dataset[\"test\"][\"text\"]), cast(Column, dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec79138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of predictions for static keyword analyzer\n",
    "keyword_checker = KeywordChecker()\n",
    "y_pred_keyword = []\n",
    "for prompt in X_test:\n",
    "    report = keyword_checker.analyze(prompt=prompt)\n",
    "\n",
    "    if report is None:\n",
    "        # The the keyword checker didn't find any unsafe keywords in the prompt\n",
    "        y_pred_keyword.append(0)\n",
    "    else:\n",
    "        y_pred_keyword.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd078d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of predictions for TF-IDF classifier\n",
    "tfidf_classifier = TfidfClassifier()\n",
    "y_pred_tfidf = []\n",
    "for prompt in X_test:\n",
    "    report = tfidf_classifier.analyze(prompt=prompt)\n",
    "    y_pred_tfidf.append(report.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a1dc5",
   "metadata": {},
   "source": [
    "Instead of re-running the LLM-based solution, let's load the evaluation data from a file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f526952",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"\n",
    "    Represents the results of a model evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true: list[int]\n",
    "    y_pred: list[int]\n",
    "    failed_indices: list[int]\n",
    "\n",
    "\n",
    "# Path to the saved JSON file\n",
    "# result_filepath = os.path.join(data_dir, \"llm_safety_eval_Mistral-7B-Instruct-v0.3.json\")\n",
    "result_filepath = os.path.join(data_dir, \"llm_safety_eval_Qwen3-4B-Instruct-2507.json\")\n",
    "\n",
    "filename = os.path.basename(result_filepath)\n",
    "llm_model_name = filename.replace(\"llm_safety_eval_\", \"\").replace(\".json\", \"\")\n",
    "\n",
    "# Load JSON from file\n",
    "with open(result_filepath, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Rebuild EvaluationResult instance\n",
    "llm_result = EvaluationResult(y_true=data[\"y_true\"], y_pred=data[\"y_pred\"], failed_indices=data[\"failed_indices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3344d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since failed indices are not included in llm_result.y_pred in the evaluation report,\n",
    "#  we need to insert zeros at those positions to reconstruct the full list of LLM predictions\n",
    "y_pred_llm = []\n",
    "failed_set = set(llm_result.failed_indices)  # For O(1) lookups\n",
    "llm_iter = iter(llm_result.y_pred)\n",
    "\n",
    "for i in range(len(llm_result.y_pred) + len(llm_result.failed_indices)):\n",
    "    if i in failed_set:\n",
    "        y_pred_llm.append(0)\n",
    "    else:\n",
    "        y_pred_llm.append(next(llm_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions from all components: mark 1 if any component predicted 1, otherwise 0\n",
    "assert len(y_pred_llm) == len(y_pred_tfidf) == len(y_pred_keyword) == len(y_test)\n",
    "y_pred_system = [\n",
    "    1 if llm == 1 or tfidf == 1 or keyword == 1 else 0\n",
    "    for llm, tfidf, keyword in zip(y_pred_llm, y_pred_tfidf, y_pred_keyword)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477f921",
   "metadata": {},
   "source": [
    "With the unified predictions for the complete system now loaded, let's generate the performance metrics report and the corresponding confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(y_pred: list[int], y: Column, title: str):\n",
    "    \"\"\"\n",
    "    Generate confusion matrix.\n",
    "    \"\"\"\n",
    "    labels = [\"Safe (0)\", \"Unsafe (1)\"]\n",
    "    cm = confusion_matrix(y, y_pred, labels=[0, 1])\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=\"Blues\",\n",
    "            hoverongaps=False,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text}\",\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Count\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"True Label\",\n",
    "        yaxis=dict(autorange=\"reversed\"),\n",
    "        width=580,\n",
    "        height=500,  # Make the plot square\n",
    "        margin=dict(l=80, r=80, t=100, b=80),\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e780ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = generate_confusion_matrix(\n",
    "    y_pred=y_pred_system,\n",
    "    y=y_test,\n",
    "    title=f\"Test Set Confusion - System with {llm_model_name}\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39c495",
   "metadata": {},
   "source": [
    "Here we see that the The Mistral-based system achieved an overall accuracy of 94%, compared to 98% for the Qwen-based system. Both models attained near-perfect recall for unsafe prompts, with the primary performance difference observed in precision for unsafe prompts. The Qwen-based system achieved 96% precision (30 misclassifications), whereas the Mistral-based system achieved 84% precision (120 misclassifications). These results indicate that we should proceed with the Qwen-based system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix to file for use in the report\n",
    "html_str = f\"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "  {fig.to_html(full_html=False, include_plotlyjs='cdn')}\n",
    "</div>\n",
    "\"\"\"  # noqa: E702, E222\n",
    "output_file = os.path.join(plots_dir, f\"conf_matrix_system_test_{llm_model_name}.html\")\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(html_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
